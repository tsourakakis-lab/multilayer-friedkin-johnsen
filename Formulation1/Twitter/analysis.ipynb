{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c629e982",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time, timezone\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d50da6",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711848c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('vax_final.json') as json_file:\n",
    "    vax_data = json.load(json_file) #vaccination\n",
    "    \n",
    "with open('ukrainewar_final.json') as json_file:\n",
    "    war_data = json.load(json_file) #war"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d8e47",
   "metadata": {},
   "source": [
    "## Labelling Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique integer for each date\n",
    "def make_timestamp(x):\n",
    "    x = x['created_at']\n",
    "    return 12*24*40*(x['year']-2019) + 24*30*x['month'] + 24*x['day'] + x['hour'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061612ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TweetID -> opinion rating\n",
    "tid_vax_label = pickle.load(open(\"vax_gpt_labels.pkl\",'rb'))\n",
    "tid_war_label = pickle.load(open(\"war_gpt_labels.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b162b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with keys: user_ids \n",
    "#           and values: list of triplets (tweet_id, timestamp, rating)\n",
    "vax_nodeid_to_tweetsids = {}\n",
    "\n",
    "vax_min_timestamp = 10000*23000 # min timestamp\n",
    "vax_max_timestamp = 0           # max timestamp\n",
    "for nodeid in vax_data:\n",
    "    vax_nodeid_to_tweetsids[nodeid] = []\n",
    "    for el in vax_data[nodeid]:\n",
    "        t = make_timestamp(el)\n",
    "        vax_min_timestamp = min(vax_min_timestamp, t)\n",
    "        vax_max_timestamp = max(vax_max_timestamp, t)\n",
    "        vax_nodeid_to_tweetsids[nodeid].append([el['tweet_id'], t])\n",
    "    vax_nodeid_to_tweetsids[nodeid] = sorted(vax_nodeid_to_tweetsids[nodeid], key=lambda x: x[1])\n",
    "    for i in range(len(vax_nodeid_to_tweetsids[nodeid])):\n",
    "        opinion = 5\n",
    "        if i>0: opinion = vax_nodeid_to_tweetsids[nodeid][i-1][2]\n",
    "        id_tweet = vax_nodeid_to_tweetsids[nodeid][i][0]\n",
    "        if id_tweet in tid_vax_label: opinion = tid_vax_label[id_tweet]\n",
    "        vax_nodeid_to_tweetsids[nodeid][i].append(opinion)\n",
    "\n",
    "# Dictionary with keys: user_ids \n",
    "#           and values: list of triplets (tweet_id, timestamp, rating)\n",
    "war_nodeid_to_tweetsids = {}\n",
    "\n",
    "war_min_timestamp = 10000*23000 # min timestamp\n",
    "war_max_timestamp = 0           # max timestamp\n",
    "for nodeid in war_data:\n",
    "    war_nodeid_to_tweetsids[nodeid] = []\n",
    "    for el in war_data[nodeid]:\n",
    "        t = make_timestamp(el)\n",
    "        war_min_timestamp = min(war_min_timestamp, t)\n",
    "        war_max_timestamp = max(war_max_timestamp, t)\n",
    "        war_nodeid_to_tweetsids[nodeid].append([el['tweet_id'], t]) \n",
    "    war_nodeid_to_tweetsids[nodeid] = sorted(war_nodeid_to_tweetsids[nodeid], key=lambda x: x[1])\n",
    "    for i in range(len(war_nodeid_to_tweetsids[nodeid])):\n",
    "        opinion = 5\n",
    "        if i>0: opinion = war_nodeid_to_tweetsids[nodeid][i-1][2]\n",
    "        id_tweet = war_nodeid_to_tweetsids[nodeid][i][0]\n",
    "        if id_tweet in tid_war_label: opinion = tid_war_label[id_tweet]\n",
    "        war_nodeid_to_tweetsids[nodeid][i].append(opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88163429",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vax_min_timestamp, vax_max_timestamp)\n",
    "print(war_min_timestamp, war_max_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e22a84",
   "metadata": {},
   "source": [
    "# Who-Follows-Whom Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b60996",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"final.edgelist\", create_using=nx.Graph)\n",
    "G.to_undirected()\n",
    "print(f'Number of nodes: {len(G.nodes())}')\n",
    "print(f'Number of edges: {len(G.edges())}')\n",
    "plt.hist(list(dict(G.degree).values()), bins=50)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Who-Follows-Whom - Degree Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index and inverted index to map twitter_node_id to an integer index \n",
    "ids_to_index = {}\n",
    "index_to_ids = {}\n",
    "counter = 0\n",
    "for node in G.nodes():\n",
    "    ids_to_index[node] = counter\n",
    "    index_to_ids[counter] = node\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save graph\n",
    "with open(\"G.edgelist\", \"w\") as f:\n",
    "    for edge in G.edges():\n",
    "        u = ids_to_index[edge[0]]\n",
    "        v = ids_to_index[edge[1]]\n",
    "        f.write(f'{u} {v}\\n')\n",
    "        f.write(f'{v} {u}\\n') #remove that for directed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab1b51",
   "metadata": {},
   "source": [
    "## Making Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of timestamps\n",
    "T = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting a list a to n buckets\n",
    "def equal_split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "#spliting tweets int T buckets that correspond to a single timestamp\n",
    "#all buckets contain the same number of tweets\n",
    "def make_pairs(G, nodeid_to_tweetsids, T = 20):\n",
    "    allts = []\n",
    "    for nodeid in G.nodes():\n",
    "        list_el = nodeid_to_tweetsids[nodeid]\n",
    "        for i in range(len(list_el)): allts.append(list_el[i][1])\n",
    "    list_el = sorted(allts)\n",
    "    res = list(equal_split(list_el,T))\n",
    "    pairs = []\n",
    "    for el in res: pairs.append([el[0],el[-1]])\n",
    "    return pairs\n",
    "\n",
    "#making the opinions for each user in each timestamp\n",
    "def make_opinions(G, ids_to_index, nodeid_to_tweetsids, pairs, filname='./test1.txt'):\n",
    "    #an nxT matrix, with value of (i,j) element being the opinion rating of node i at time j\n",
    "    ids_to_score = np.zeros((len(G.nodes), len(pairs)))\n",
    "    #for each node\n",
    "    for nodeid in G.nodes():\n",
    "        #index of node\n",
    "        index = ids_to_index[nodeid]\n",
    "        #list of triplets (tweet_id, timestamp, rating)\n",
    "        list_el = nodeid_to_tweetsids[nodeid]\n",
    "        i = 0\n",
    "        curall = []\n",
    "        #each timestamp t corresponds to a period [tmin, tmax]\n",
    "        for t in range(len(pairs)):\n",
    "            tmin = pairs[t][0]\n",
    "            tmax = pairs[t][1]\n",
    "            while True:\n",
    "                if list_el[i][1]>= tmax: \n",
    "                    #if the user with id = index didn't make any tweet at period t = [tmin, tmax]\n",
    "                    if len(curall)==0: \n",
    "                        #if t>0 then set the opinion of t-1\n",
    "                        if t>0: ids_to_score[index][t] = ids_to_score[index][t-1]\n",
    "                        #if t=0 set initial opinion neutral equal to 5\n",
    "                        else: ids_to_score[index][t] = 5 \n",
    "                    #if user with id = index have made at least one tweet at period t = [tmin, tmax]\n",
    "                    else: \n",
    "                        #set as opinion the average of the tweet's labeling\n",
    "                        ids_to_score[index][t] = np.sum(curall)/len(curall)\n",
    "                    curall = []\n",
    "                    break\n",
    "                elif list_el[i][1]>= tmin:\n",
    "                    curall.append(list_el[i][2])\n",
    "                if (i<(len(list_el)-1)): i = i+1\n",
    "                else: \n",
    "                    ids_to_score[index][t] = ids_to_score[index][t-1]\n",
    "                    break\n",
    "    np.savetxt(filname, ids_to_score, delimiter=' ')\n",
    "    return ids_to_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01192581",
   "metadata": {},
   "source": [
    "### Saving Opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vax_pairs = make_pairs(G, vax_nodeid_to_tweetsids, T)\n",
    "vax_ids_to_score = make_opinions(G, ids_to_index, vax_nodeid_to_tweetsids, vax_pairs, filname='./vax_opinions.txt')\n",
    "\n",
    "war_pairs = make_pairs(G, war_nodeid_to_tweetsids, T)\n",
    "war_ids_to_score = make_opinions(G, ids_to_index, war_nodeid_to_tweetsids, war_pairs, filname='./war_opinions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(vax_ids_to_score)\n",
    "plt.bar(range(1,T+1), np.sum(vax_ids_to_score, 0)/n)\n",
    "plt.title('Distribution of averaged vax-opinions over time')\n",
    "plt.show()\n",
    "\n",
    "n = len(war_ids_to_score)\n",
    "plt.title('Distribution of averaged war-opinions over time')\n",
    "plt.bar(range(1,T+1), np.sum(war_ids_to_score, 0)/n)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aed8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "    bucket = [0 for i in range(11)]\n",
    "    for i in range(len(vax_ids_to_score)):\n",
    "        bucket[int(vax_ids_to_score[i,t])]+=1\n",
    "    plt.plot(bucket)\n",
    "plt.title(\"Vax Tweets - Opinion Distribution\")\n",
    "plt.show()\n",
    "\n",
    "for t in range(T):\n",
    "    bucket = [0 for i in range(11)]\n",
    "    for i in range(len(war_ids_to_score)):\n",
    "        bucket[int(war_ids_to_score[i,t])]+=1\n",
    "    plt.plot(bucket)\n",
    "plt.title(\"War Tweets - Opinion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84465ba5",
   "metadata": {},
   "source": [
    "# Vax-Mention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "vax_mention = pickle.load(open(\"vax_mention_tid_dict.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes of all user_ids included in the mention-data\n",
    "all_vax_ids = set()\n",
    "for el in vax_mention:\n",
    "    all_vax_ids.add(el)\n",
    "    for el2 in vax_mention[el]:\n",
    "        all_vax_ids.add(el2)\n",
    "all_vax_ids = [str(el) for el in list(all_vax_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d703f",
   "metadata": {},
   "source": [
    "### Making the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "vax_mention_edges = [(i,j) for i in vax_mention for j in vax_mention[i]]\n",
    "G_vax = nx.Graph()\n",
    "G_vax.add_edges_from(vax_mention_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ced78",
   "metadata": {},
   "source": [
    "### LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ac791",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    largest_cc = max(nx.connected_components(G_vax), key=len)\n",
    "    print(len(largest_cc))\n",
    "    print(largest_cc)\n",
    "    G_vax = nx.induced_subgraph(G_vax, largest_cc).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659528c",
   "metadata": {},
   "source": [
    "### Find the induced subgraph of G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a71ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_v = nx.induced_subgraph(G, all_vax_ids).copy()\n",
    "G_v.to_undirected()\n",
    "print(G_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index and inverted index to map twitter_node_id to an integer index \n",
    "ids_to_indexvax = {}\n",
    "indexvax_to_ids = {}\n",
    "counter = 0\n",
    "for node in all_vax_ids:\n",
    "    ids_to_indexvax[node] = counter\n",
    "    indexvax_to_ids[counter] = node\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e2ce8",
   "metadata": {},
   "source": [
    "### Saving the 2-Layer vax-networks and nodes opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "with open(\"./G_vax_scc.edgelist\", \"w\") as f:\n",
    "    for edge in G_vax.edges():\n",
    "        u = ids_to_indexvax[str(edge[0])]\n",
    "        v = ids_to_indexvax[str(edge[1])]\n",
    "        if G_v.has_edge(str(edge[0]), str(edge[1]))==False: cont+=1\n",
    "        f.write(f'{u} {v}\\n')\n",
    "        f.write(f'{v} {u}\\n') #remove it for directeds\n",
    "print(f'There are {cont} out of {len(G_vax.edges())} edges not included in the other layer...')        \n",
    "  \n",
    "cont = 0\n",
    "with open(\"./G_v_scc.edgelist\", \"w\") as f:\n",
    "    for edge in G_v.edges():\n",
    "        u = ids_to_indexvax[str(edge[0])]\n",
    "        v = ids_to_indexvax[str(edge[1])]\n",
    "        if G_vax.has_edge(edge[0], edge[1])==False: cont+=1\n",
    "        f.write(f'{u} {v}\\n')\n",
    "        f.write(f'{v} {u}\\n') #remove it for directed\n",
    "print(f'There are {cont} out of {len(G_v.edges())} edges not included in the other layer...')        \n",
    "\n",
    "#Opinions\n",
    "vax_pairs = make_pairs(G_v, vax_nodeid_to_tweetsids, T)\n",
    "vax_ids_to_score = make_opinions(G_v, ids_to_indexvax, vax_nodeid_to_tweetsids, vax_pairs, filname='./vax_ops.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bd2c6",
   "metadata": {},
   "source": [
    "# War-Mention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "war_mention = pickle.load(open(\"war_mention_tid_dict.pkl\",\"rb\"))\n",
    "for el in war_mention: \n",
    "    for i in range(len(war_mention[el])): \n",
    "        war_mention[el][i] = int(war_mention[el][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes of all user_ids included in the mention-data\n",
    "all_war_ids = set()\n",
    "for el in war_mention:\n",
    "    all_war_ids.add(el)\n",
    "    for el2 in war_mention[el]:\n",
    "        all_war_ids.add(el2)\n",
    "all_war_ids = [str(el) for el in list(all_war_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39530151",
   "metadata": {},
   "source": [
    "### Making the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "war_mention_edges = [(i,j) for i in war_mention for j in war_mention[i]]\n",
    "G_war = nx.Graph()\n",
    "G_war.add_edges_from(war_mention_edges)\n",
    "print(G_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8160f",
   "metadata": {},
   "source": [
    "### LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67def9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    largest_cc = max(nx.connected_components(G_war), key=len)\n",
    "    print(len(largest_cc))\n",
    "    print(largest_cc)\n",
    "    G_w = nx.induced_subgraph(G_war, largest_cc).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b38ffc",
   "metadata": {},
   "source": [
    "### Find the induced subgraph of G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_w = nx.induced_subgraph(G, all_war_ids).copy()\n",
    "G_w = G_w.to_undirected()\n",
    "print(G_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index and inverted index to map twitter_node_id to an integer index \n",
    "ids_to_indexwar = {}\n",
    "indexwar_to_ids = {}\n",
    "counter = 0\n",
    "for node in all_war_ids:\n",
    "    ids_to_indexwar[node] = counter\n",
    "    indexwar_to_ids[counter] = node\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb4c1b",
   "metadata": {},
   "source": [
    "### Saving the 2-Layer vax-networks and nodes opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a334198",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "with open(\"./G_war_scc.edgelist\", \"w\") as f:\n",
    "    for edge in G_war.edges():\n",
    "        u = ids_to_indexwar[str(edge[0])]\n",
    "        v = ids_to_indexwar[str(edge[1])]\n",
    "        if G_w.has_edge(str(edge[0]), str(edge[1]))==False: cont+=1\n",
    "        f.write(f'{u} {v}\\n')\n",
    "        f.write(f'{v} {u}\\n') #remove it for directed\n",
    "print(f'There are {cont} out of {len(G_war.edges())} edges not included in the other layer...')        \n",
    "        \n",
    "with open(\"./G_w_scc.edgelist\", \"w\") as f:\n",
    "    for edge in G_w.edges():\n",
    "        u = ids_to_indexwar[str(edge[0])]\n",
    "        v = ids_to_indexwar[str(edge[1])]\n",
    "        if G_war.has_edge(str(edge[0]), str(edge[1]))==False: cont+=1\n",
    "        f.write(f'{u} {v}\\n')\n",
    "        f.write(f'{v} {u}\\n') #remove it for directed\n",
    "print(f'There are {cont} out of {len(G_w.edges())} edges not included in the other layer...')        \n",
    "\n",
    "#Opinions\n",
    "war_pairs = make_pairs(G_w, war_nodeid_to_tweetsids, T)\n",
    "war_ids_to_score = make_opinions(G_w, ids_to_indexwar, war_nodeid_to_tweetsids, war_pairs, filname='./war_ops.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9d9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1daf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
